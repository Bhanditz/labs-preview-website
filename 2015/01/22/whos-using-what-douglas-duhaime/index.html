<p>This week’s “Who’s Using What” spotlight goes to <a href="http://douglasduhaime.com/">Douglas
Duhaime</a>; from the University of Notre Dame.
Douglas’s research takes him to intersections of early modern
natural philosophy and classical political economy in eighteenth-century
literary works.  Douglas has been pursuing the relationship between
these two facets by using natural language processing techniques,
running hand-written scripts on text data from <a href="http://eebo.chadwyck.com/home">Early
English Books Online</a>; (EEBO), <a href="http://gdc.gale.com/products/eighteenth-century-collections-online/">Enlightenment Century Collections Online</a>; (EECO) and the <a href="http://rstl.royalsocietypublishing.org/">Philosophical Transactions</a>; looking to trace patterns in
Enlightenment-era literary history. </p>

<p>You can follow Douglas on <a href="https://twitter.com/douglasduhaime">Twitter</a>, <a href="https://github.com/duhaime">Github</a>, and DHcommons and gather more insight into his work on his <a href="http://douglasduhaime.com">website</a>.</p>

<h2 id="what-open-source-tools-are-you-currently-working-with">1. What open source tools are you currently working with?</h2>

<p>I'm currently working on a few different text
mining projects, a few of which use libraries like <a href="http://www.cs.waikato.ac.nz/ml/weka/">WEKA</a>; and
<a href="http://scikit-learn.org/stable/">scikit-learn</a>; for analysis and <a href="http://ggplot2.org/">ggplot</a>; or <a href="https://github.com/mbostock/d3">D3</a>; for visualization. Because a lot of my work
revolves around natural language processing, tools like the <a href="http://nlp.stanford.edu/software/corenlp.shtml">Stanford NLP Pipeline</a>
, <a href="http://wordnet.princeton.edu/">Princeton's WordNet</a>, and the <a href="http://snowball.tartarus.org/">Snowball stemmer</a>; are standard resources. When working with
early modern texts, I've also been using <a href="http://ucrel.lancs.ac.uk/vard/about/">Vard2</a>; and the
variant tables in the <a href="http://morphadorner.northwestern.edu/">MorphAdorner</a>; package for orthographical
normalization.</p>

<h2 id="what-open-source-tools-have-you-used-in-the-past-to-develop-larger-applications">What open source tools have you used in the past to develop larger applications?</h2>

<p>A few of the tools
I've built draw upon <a href="http://www.seleniumhq.org/">Selenium's</a>; browser automation framework–which handles
well in Javascript and AJAX-rich environments–and <a href="https://pypi.python.org/pypi/Whoosh/">Whoosh's</a>; 
full text indexing functions. Almost everything I write draws upon
<a href="http://www.nltk.org/">Python's Natural Language Tool Kit</a>; and BeautifulSoup at
some point. When working on applications that require fuzzy string
matching, I like to use <a href="https://docs.python.org/2/library/difflib.html">difflib</a></p>

<h2 id="what-are-you-currently-developing">3. What are you currently developing?</h2>

<p>I'm working with
others in the University of Notre Dame's Text Mining Working Group
to develop an open source web service capable of identifying literary
allusions in user-provided texts. The prototype runs custom algorithms
against an SQLite database with a Django back-end, and uses many of the
text processing libraries discussed above.</p>

<h2 id="what-would-you-like-to-see-developed">4. What would you like to see developed?</h2>

<p>I would like to see a library
capable of estimating the likelihood that a given text contains one or
more simple ciphers (substitution or null, for instance). Such a
resource would be highly useful in certain contexts. I also eagerly
await both the <a href="http://emop.tamu.edu/">EMOP</a>; team's OCR engine for early modern typography and a
package capable of achieving high sentiment classification scores on
older prose. Both will be very valuable tools.</p>
